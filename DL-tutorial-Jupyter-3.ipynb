{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorials - Chapter 3 - Multi-layer perceptron and back-propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Perceptron\n",
    "\n",
    "- The first mathematical model for a neuron was the threshold logic unit, with boolean inputs and outputs (logic gates)\n",
    "- **Perceptrons** are like these basic binary neuron models except they can operate on any values. Inspired by biology. With $w_{i}$ being synaptic weights and $x_{i}$ and $f$ firing rates. However, it is a very crude biological model\n",
    "\n",
    "Perceptrons can be modeled mathematically as the following:\n",
    "\n",
    "![per](./images/perceptron.png)\n",
    "\n",
    "$$f(x) = \\sigma (w*x+b)$$\n",
    "\n",
    "The following mathematical model can be visualized like this:\n",
    "\n",
    "![mm](./images/math-model.png)\n",
    "\n",
    "This model is simplified when we consider $x, w, b$ to be tensors\n",
    "\n",
    "![tm](./images/tensor-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here is a training algorithm for our perceptrons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(x,y, nb_epochs_max):\n",
    "    w = torch.zeros(x.size(1)) \n",
    "\n",
    "    for e in range(nb_epochs_max):\n",
    "        nb_changes = 0\n",
    "        for i in range(x.size(0)):\n",
    "            if x[i].dot(w) * y[i] <= 0:\n",
    "                w = w + y[i] * x[i]\n",
    "                nb_changes += 1\n",
    "            if nb_changes == 0: break;\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Layer of a Multilayer perceptron in tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MyDenseLayer,self).__init__()\n",
    "\n",
    "        # Intiailze weights and biases\n",
    "        self.W = self.add_weight([input_dim, output_dim])\n",
    "        self.b = self.add_weight([1, output_dim])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # forward propogate our inputs\n",
    "        z = tf.matmul(inputs,self.W) + self.b\n",
    "\n",
    "        # feed through non-linearity\n",
    "        output = tf.math.sigmoid(z)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **dense** layer is called a dense layer because every input is connected to every output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-The perceptron stops as soon as it finds a separating boundary.\n",
    "- Other algorithms maximize the distance of samples to the decision boundary, which improves the robustness to noise.\n",
    "- Support Vector Machines (SVM) achieve this by minimizing \n",
    "\n",
    "$$L(w,b) = \\lambda ||w||^{2} + \\frac{1}{N} \\sum_{n} max(0,1 - y_{n} (w*x_{n}+ b))$$\n",
    "\n",
    "which is convex and has a global optimum\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hinge Loss\n",
    "\n",
    "the term \n",
    "\n",
    "$$max(0,1 - \\alpha)$$\n",
    "\n",
    "is called the \"hinge loss\"\n",
    "\n",
    "![hinge](./images/hinge.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus MIT Introduction to Deep Learning | 6.S191\n",
    "\n",
    "**Artificial Intellegence**: Any automated task to teach a computer to mimic human behavior\n",
    "\n",
    "**Machine Learning**: Teaching a machine to learn without explicitly telling it to. Teach a computer through experiences. (Typically have to manually select a feature space which is big challenge)\n",
    "\n",
    "**Deep Learning**: Learning both the features and the learning from raw data\n",
    "\n",
    "#### Activation function\n",
    "\n",
    "- **Activation functions** are in the essence of every deep learning model. Their main purpose is to introduce non-linearity to fit non-linear representations to our data. This activation function is what makes deep learning so powerful. \n",
    "\n",
    "How to build a deep neural network model using tensorflow\n",
    "\n",
    "![dnn](./images/dnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(n,),\n",
    "    tf.keras.layers.Dense(n,),\n",
    "    tf.keras.layers.Dense(n,),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss\n",
    "\n",
    "More definitions\n",
    "- The **Empirical Loss** measures the total loss over the entire dataset\n",
    "\n",
    "Goal is to minimize empirical loss\n",
    "\n",
    "Use cases:\n",
    "- Softmax cross entropy loss function = binary classification\n",
    "- Mean squared error loss = regresiion models that outputs continuous real numbers\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "Algorithm to find optimal solutions iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow s tf\n",
    "\n",
    "weights = tf.Variable([tf.random.normal()])\n",
    "\n",
    "while True: # loops forever\n",
    "    with tf.GradientTape() as g:\n",
    "        loss = compute_loss(weights)\n",
    "        gradient = g.gradient(loss, weights)\n",
    "    \n",
    "    weights = weights -lr * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rates (`lr`) are important in gradient descent \n",
    "\n",
    "- small learning rates may converge at incorrect local minima\n",
    "- large learnig rare may overshoot, become unstable, and diverge\n",
    "\n",
    "examples of gradient descent algorithms \n",
    "\n",
    "- SGD `tf.keras.optimizer.SGD`\n",
    "- Adam `tf.keras.optimizer.Adam`\n",
    "- Adadelta `tf.keras.optimizer.Adadelta`\n",
    "- Adagrad `tf.keras.optimizer.Adagrad`\n",
    "- RMSProp `tf.keras.optimizer.RMSProp`\n",
    "\n",
    "Putting whole lesson together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([...])\n",
    "\n",
    "# pick your favorite optimizer\n",
    "optimizer = tf.keras.optimizers.SGD()\n",
    "\n",
    "while True:\n",
    "\n",
    "    # forward pass through the network\n",
    "    prediction = model(x)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # compute loss\n",
    "        loss = compute_loss(y, prediction)\n",
    "    \n",
    "    # update the weights using the gradient\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to combat overfitting since it is a big issue in deep learning: **Regularization**\n",
    "\n",
    "**Regularization I**: Dropout\n",
    "- During training, randomly set some activations to 0\n",
    "- typically drop 50% of activations in layer\n",
    "- forces network to not rely on any I node\n",
    "- `tf.keras.layers.Dropout(p=0.5)`\n",
    "\n",
    "**Regularization II**: Early Stopping\n",
    "- stop training before we have a chance to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Probabilistic view of a linear classifier\n",
    "\n",
    "The Linear Discriminant Analysis (LDA) algorithm provides a nice bridge between these linear classifiers and probabilstic modeling\n",
    "\n",
    "Consider the twoo following populations: \n",
    "\n",
    "![2pop](./images/2pop.png)\n",
    "\n",
    "That is, they are Gaussian with the **same covariance matrix** $\\Sigma$. This is the homoscedasticity assumption\n",
    "\n",
    "Intutively, we can map data linearly to make all the covariance matrices identity, there the Bayesian separation is a plan, so it is also in the original space.\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "signmoid function: soft heavy side function, popular loss function\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Linear Separability and feature design\n",
    "\n",
    "The main weakness of linear predictors is their lack of capacity. For classification, the populations have to be linearly separable. Basically its difficult to model data with only linear lines. \n",
    "\n",
    "Training a model composed of manually engineered features and parametric model such as logistic regression is now referred to as \"shallow learning\"\n",
    "\n",
    "The signal goes through a single processing trained from data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Multilayer Perceptron\n",
    "\n",
    "Even though it has no practical value implementation-wise, we can represent such a model as a combination of units\n",
    "\n",
    "![mlp101](./images/mlp101.png)\n",
    "\n",
    "#### Activation functions \n",
    "\n",
    "An activation function introduces non-linearity to our model. Here are the two most common\n",
    "\n",
    "![activation](./images/act.png)\n",
    "\n",
    "#### ReLU\n",
    "\n",
    "ReLU activation function is a piecewise linear combination that can approximate any function through a linear combinations of straight lines at diferent translated/scaled RELU functions. Below you can see this polynomial being fitted with these RELUs:\n",
    "\n",
    "![relu](./images/Relu.png)\n",
    "\n",
    "#### General notes on training/testing\n",
    "\n",
    "- **False Misconception:** a better approximation requires a larger hiiden layer (larger `k`) and this theorem says nothing about the relation between the two\n",
    "- so this results states that we can make the training error as low as we want by using a larger hidden layer. it states nothing about the test error\n",
    "- deploying mlp in practice is often a balanceing act between under-fitting and over-fitting but we can combat this through regularization (dropout, early stopping)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Gradient Descent\n",
    "\n",
    "Gradient descent is an iterative algorithm used to find local minima of a given space. Refer to MIT notes for more details\n",
    "\n",
    "Python implementation in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x,y,w,b):\n",
    "    u = y * y( -y * (x @ w + b)).sigmoid()\n",
    "    v = x * u.view(-1,1)\n",
    "    return v.sum(0), -u.sum(0)\n",
    "\n",
    "# Gradient descent algorithm\n",
    "w, b = torch.empty(x.size(1)).normal_(), 0 # weights and biases\n",
    "eta = 1e-1 # learning rate\n",
    "\n",
    "for k in range(nb_iterations):\n",
    "    print(k, loss(x,y,w,b))\n",
    "    dw, db = gradient(x,y,w,b)\n",
    "    w -= eta * dw\n",
    "    b -= eta * db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 Back Propogation\n",
    "\n",
    "MLP & deep learning in general relies on back propogation to compute the values during gradient descent\n",
    "\n",
    "![backprop](./images/backprop.png)\n",
    "\n",
    "![help](./images/helpful-3.6.png)\n",
    "\n",
    "- Backward pass is a simple algorithm: apply the chain rule again and again\n",
    "\n",
    "- Forward pass, can be expressed in tensorial form. Heavy computation is concentrated in linear operations, and all the non-linearities go into component-wise operations\n",
    "\n",
    "- Without tricks, we have to keep in memory, all the activations computed during the forward pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ada')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b47ca7d216b8a465b979ff7f012da33d157a8eca88a6176851bc787c68093159"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
