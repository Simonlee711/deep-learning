{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorials - Chapter 2 - ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 - Loss and Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- The general objective in ML is to capture regularity in data to make predictions\n",
    "- **Learning** consists of finding in a set $F$ of functionals a \"good\" $f*$ usually defined through a loss\n",
    "\n",
    "### Important definitions:\n",
    "- **loss** - average errror over training data\n",
    "- **risk** - average error over all data\n",
    "\n",
    "$$ l: F \\text{x} L -> R$$\n",
    "\n",
    "- We are looking for an f with a small expected risk. So if we minimize risk, then we have obtained an optimal $f*$\n",
    "\n",
    "$$f* = argmin R(f)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Over and under fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overfitting, when model is too specific to data.\n",
    "- Underfitting, where model is too generic to data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Bias Variance Dilemma\n",
    "\n",
    "![c](./images/bias-variance-dil.png)\n",
    "\n",
    "- **Degrees of Freedom** in Machine Learning. In predictive modeling, the degrees of freedom often refers to the number of parameters in the model that are estimated from data. This can also include both the coefficients of the model and the data used in the calculation of the error of the model.\n",
    "\n",
    "- Conceptually model-fitting and regularisation can be interpreted as bayesian inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Evaluation Protocols\n",
    "\n",
    "- Learning algoritghms, in DL , require the tuning of many meta-parameters\n",
    "- These parameteres have a strong impact on the performance, resulting in a meta over-fitting through experiments\n",
    "- we must be extra careful with performance estimation\n",
    "\n",
    "![dev](./images/dev-cycle.png)\n",
    "\n",
    "**However the standard for us is to have a separate validation set for the tuning.**\n",
    "\n",
    "- When data is scarce, one can use cross validation: average through multiple random splits of the data in a train and validation sets\n",
    "- There is no unbiased stimator of the variance of cross-validation valid under all distributions\n",
    "\n",
    "#### Things to avoid in ML\n",
    "\n",
    "- Early stopping evaluation\n",
    "- meta-parameter (over) tuning\n",
    "- data-set selection\n",
    "- algorithm data-set specific clauses\n",
    "- seed selection\n",
    "\n",
    "The ML community pushes toward accessible implmenetations, reference data-sets, leader boards, and constant upgrades of benchmarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Basic Embeddings\n",
    "\n",
    "Deep Learning models combine embeddings and dimesnion reduction operations. They parameterize and re-parametreize mutliple times the input signal into representations that get more and more invariant and noise free. To get an intutition how this is possible, we consider here two standard algorithms:\n",
    "\n",
    "1. K-means (Lloyd's algorithm)\n",
    "2. PCA\n",
    "\n",
    "#### Lloyd's algorithm\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "![lloyd](./images/lloyd.png)\n",
    "\n",
    "What K means does in practice with MNST dataset:\n",
    "\n",
    "![kmean](./images/pca-kmeans.png)\n",
    "\n",
    "#### Principal Components Analysis\n",
    "\n",
    "Eigendecomposition method that tries to represent the data in a linear way by finding the orthogonal points. Principal components are a hyperparameter. \n",
    "\n",
    "![pca](./images/PCA-mnst.png)\n",
    "\n",
    "#### Significance\n",
    "\n",
    "- these results show that even crude embeddings capture something meaningful. Changes in pixel intensity as expected but also deformations in the \"indexing\" space (the image plane).\n",
    "- However, translations and deformations damage the representation badly and \"composition\" (object on background) is not handled at all\n",
    "\n",
    "#### Use cases of embeddings in DL\n",
    "\n",
    "We would like\n",
    "- to use many encodings \" of these sorts\" for small local structures with limited variability\n",
    "- have different \"channels\" for different components\n",
    "- process at multiple scales\n",
    "\n",
    "Computationally, we would like to deal with large signals and large training sets, so we need to avoid super-linear cost in one or the other"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
