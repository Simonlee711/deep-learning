{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorials - Chapter 4 - Graphs of operators, autograd, and convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 DAG-Networks\n",
    "\n",
    "We can generalize an MLP to an arbitrary \"Directed Acyclic Graph\" operator. Moreso we can build an arbitrary directed acyclic graph with these operators at the nodes, compute the response of the resulting mapping and compute its gradient with back propogation.\n",
    "\n",
    "![dag](./images/dag.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for the above dag\n",
    "import tensorflow as tf\n",
    "w1 = tf.Variable(tf.random_normal([5,5]))\n",
    "w2 = tf.Variable(tf.random_normal([5,5]))\n",
    "x = tf.Variable(tf.random_normal([5,1]))\n",
    "x0 = x\n",
    "x1 = tf.matmul(w1, x0)\n",
    "x2 = x0 + tf.matmul(w2,x1)\n",
    "x3 = tf.matmul(w1,x1+x2)\n",
    "q = tf.norm(x3)\n",
    "\n",
    "gw1, gw2 = tf.gradients(q, [w1, w2])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_intializer())\n",
    "    _gw1, _gw2 = sess.run([gw1, gw2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our generalized DAg formulation, we have in particular implicitly allowed the same parameters to modulate different parts of the processing. For instance $w^{1}$ in our example paramterizes both $\\phi^{1}$ and $\\phi^{3}$. This is called weight sharing.\n",
    "\n",
    "Weight sharing allows in particular to build **Siamese Networks** where a full sub-network is replicated several times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Autograd\n",
    "\n",
    "Conceptually, the forward pass is a standard tensor computation, and the DAG of the tensor operations is required only to compute derivatives\n",
    "\n",
    "When executing tensor operatioms, PyTorch can automatically constrcut on-the-fly the graoh of operations to compute the gradient of any quantify with respect to any tensors involved.\n",
    "\n",
    "This **autograd** mechanism has two main benefits:\n",
    "\n",
    "1. simpler syntax: one just needs to write the forward pass as a standard sequence of python operations\n",
    "\n",
    "2. Greater flexibility: since the graph is not static, the forward pass can be dynamically modulated\n",
    "\n",
    "A `Tensor` has a Boolean Field `requires_grad` set to `False` by default, which states if PyTorch should build the graph operations so that gradients with respect to it can be computed.\n",
    "\n",
    "The results of a tensorial operation has this flag to `True` if any of its operand has it to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor([1.,2.,])\n",
    "y = torch.tensor([4.,5.,])\n",
    "z = torch.tensor([7.,3.,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x+y).requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x+z).requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only floating point type tensors can have their gradient computed. \n",
    "\n",
    "`torch.autograd.grad(outputs, inputs)` computes and returns the gradient of `outputs` with respect to `inputs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 4., 8.]), tensor([0.1000, 0.0500]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([1.,2.,4.]).requires_grad_()\n",
    "u = torch.tensor([10.,20.,]).requires_grad_()\n",
    "a = t.pow(2).sum() + u.log().sum()\n",
    "torch.autograd.grad(a,(t,u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs can be a single tensor, but the result is still a one element tuple\n",
    "\n",
    "if outputs is a tuple, the results is the sum of the gradients of its elements\n",
    "\n",
    "The function `Tensor.backward` accumulates gradients in the grad fields of tensors which are not results of operations, the \"leaves\" in the autograd graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([27., 12., 75.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([-3.,2.,5.]).requires_grad_()\n",
    "u = x.pow(3).sum()\n",
    "x.grad\n",
    "u.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is an alternative to `torch.autograd.grad(...)` and standard for training models\n",
    "\n",
    "**warning** `Tensor.backward()` accumulates the gradients in the grad fields of tensors so one may have to set them to zero before calling it. \n",
    "\n",
    "This accumulating behavior is desireable to compute the gradient of a loss summed over several \"mini-batches\" or the gradient of a sum of losses\n",
    "\n",
    "![dag](./images/dag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.backward of tensor(11.2018, grad_fn=<NormBackward1>)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writing this DAG with new autograd commands\n",
    "\n",
    "w1 = torch.rand(5,5).requires_grad_()\n",
    "w2 = torch.rand(5,5).requires_grad_()\n",
    "x = torch.empty(5).normal_()\n",
    "\n",
    "x0 = x\n",
    "x1 = w1 @ x0\n",
    "x2 = x0 + w2 @ x1\n",
    "x3 = w1 @ (x1 + x2)\n",
    "\n",
    "q = x3.norm()\n",
    "\n",
    "q.backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![net](./images/network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.rand(20,10).requires_grad_()\n",
    "b1 = torch.rand(20).requires_grad_()\n",
    "w2 = torch.rand(5,20).requires_grad_()\n",
    "b2= torch.rand(5).requires_grad_()\n",
    "\n",
    "x = torch.rand(10)\n",
    "h = torch.tanh(w1 @ x + b1)\n",
    "y = torch.tanh(w2 @ h + b2)\n",
    "\n",
    "target = torch.rand(5)\n",
    "\n",
    "loss = (y-target).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![net2](./images/net2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.rand(3,10,10).requires_grad_()\n",
    "\n",
    "def blah(k,x):\n",
    "    for i in range(k):\n",
    "        x = torch.tanh(w[i] @ x)\n",
    "    return x\n",
    "\n",
    "u = blah(1, torch.rand(10))\n",
    "v = blah(3, torch.rand(10))\n",
    "q = u.dot(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**warning:** Although they are related, the autograd graph is not the networks structure, but the graph of operations to compute the gradient. It can be data-dependent and miss or replicate sub-parts of the network\n",
    "\n",
    "The `torch.no_grad()` context switches of the autograd machinery, and can be used for operations such as parameter updates\n",
    "\n",
    "The `detach()` method creates a tensor which shares the data, but does not require gradient computation, and is not connected to the current graph.\n",
    "\n",
    "This method should be used when the gradient should not be propogated beyond the variable or to update leaf tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4246, requires_grad=True) tensor(-0.4246, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = torch.tensor(0.5).requires_grad_()\n",
    "b = torch.tensor(-0.5).requires_grad_()\n",
    "eta = 1e-3\n",
    "\n",
    "for k in range(100):\n",
    "    l = (a-1)**2 + (b+1)**2 + (a-b)**2\n",
    "    ga, gb = torch.autograd.grad(l, (a,b))\n",
    "    with torch.no_grad():\n",
    "        a -= eta * ga\n",
    "        b -= eta * gb\n",
    "\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5099, requires_grad=True) tensor(-0.4901, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = torch.tensor(0.5).requires_grad_()\n",
    "b = torch.tensor(-0.5).requires_grad_()\n",
    "eta = 1e-4\n",
    "\n",
    "for k in range(100):\n",
    "    l = (a-1)**2 + (b+1)**2 + (a.detach()-b)**2\n",
    "    ga, gb = torch.autograd.grad(l, (a,b))\n",
    "    with torch.no_grad():\n",
    "        a -= eta * ga\n",
    "        b -= eta * gb\n",
    "\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, autograd deletes the computational graph when it is used. The flag `retain-graph` indicates to keep it \n",
    "\n",
    "Autograd can also track the computation of the gradient itself, to allow higher order derivatives. This specified `create_graph = True`\n",
    "\n",
    "![hod](./images/hod.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**warning**: In-place operations may corrupt values required to compute the gradient and this is tracked down by autograd\n",
    "\n",
    "They also prohibited on so-called \"leaf tensors, which are not the results of operations but the initial inputs to the whole computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 PyTorch modules and batch processing\n",
    "\n",
    "Elements from `torch.nn.functional` are autograd-compliant functions which compute a result from provided arguments alone\n",
    "\n",
    "Subclasses of `torch.nn.Module` are losses and network components. The latter embed parameters to be optimized during training.\n",
    "\n",
    "Parameters are of the type `torch.nn.Parameter` which is a `Tensor` with `requires_grad` to `True` and known to be a model parameter by various utility functions in particular `torch.nn.Module.parameters`.\n",
    "\n",
    "Usuall `torch.nn.functional` is imported as `F` and `torch.nn` as nn\n",
    "\n",
    "**warning:** functions and modules from nn process **batches** of inputs stored in tensor whose first dimension indexes them and produce corresponding tensor with the same additional dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch modules\n",
    "\n",
    "`F.relu(input, inplace=False)` takes a tensor of any size as input, applies a ReLU on each value to produce a tensor of the same size.\n",
    "\n",
    "`inplace` indicates if the operation should modify the argyment itself. This may be desirable to reduce the memory footprint of the processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8008, 0.0000, 0.5019, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "x = torch.tensor([0.8008, -0.2586, 0.5019, -0.2002, -0.7416])\n",
    "F.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Linear(in_features, out_features, bias=true)` implements a $\\real^{C} -> \\real^{D}$ fully connected layer. It takes as input a tensor size $N X C$ and proiuce a tensor size $N X D$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([4, 10])\n",
      "bias torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "f = nn.Linear(in_features=10, out_features=4)\n",
    "for n, p in f.named_parameters():\n",
    "    print(n, p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([523, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(523,10).normal_()\n",
    "y=f(x)\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.MSELoss` - implements the mean squared error loss: the sum of the component wise squared difference, divided by the total number of component in the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = nn.MSELoss()\n",
    "x = torch.tensor([[3.,]])\n",
    "y = torch.tensor([[0.,]])\n",
    "f(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2500)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[3.,0.,0.,0.]])\n",
    "y = torch.tensor([[0.,0.,0.,0.]])\n",
    "f(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter of a loss is traditionally called the **input** and the second the **target**. The two quantities may be of different dimensions or even types of some losses.\n",
    "\n",
    "#### Batch Processing\n",
    "\n",
    "Functions and modules from `nn` process samples by batches. This is motivated by the computational speed-up it induces. \n",
    "\n",
    "To evaluate a module on a sample, both the modules parameters and the sample have to be first copied into **cache memory** which is fast but small.\n",
    "\n",
    "For any model of reasonable size, only a fraction of its parameters can be kept in cache, so a modules parameters can be copied there every time it is used.\n",
    "\n",
    "**Memory transfers are slower than computation. Batch processing cuts down to one copy pf the parameters to the cache per batch**\n",
    "\n",
    "It also cuts down the use of Python loops which are awfully slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Convolutions\n",
    "\n",
    "If they were handled as normal \"unstructured\" vectors, large-dimensional signals such as sound samples or images woud require models of intractable size.\n",
    "\n",
    "For instance a linear layer taking `256 x 256` RGB image as input and producing an image of same size would require \n",
    "\n",
    "$$\n",
    "(256 x 256 x 3)^{2} \\approx 3.87e+10\n",
    "$$\n",
    "\n",
    "parameters, with corresponding memory footprint ($\\approx$ 150 Gb!) and excess of capacity\n",
    "\n",
    "Moreover this requirement is inconsistent with the intuition that such large signals have some \"invariance of translation\". A representation meaningul at a certain location can / should be used everywhere.\n",
    "\n",
    "A convolution layer embodies this idea. It applies the same linear transformation locally, everywhere and preserves the signal structure while lowering its dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical definition\n",
    "\n",
    "**$\\oplus$ indicates dot prodct**\n",
    "\n",
    "Formally in 1d, given\n",
    "\n",
    "$$\n",
    "x=(x_1,....x_W)\n",
    "$$\n",
    "\n",
    "and a \"convolution kernel\" (or \"filter\") of which $w$\n",
    "\n",
    "$$\n",
    "u = (u_1,...,u_w)\n",
    "$$\n",
    "\n",
    "the convolution $x \\oplus u$ is a vector of size $W-w+1$ with\n",
    "\n",
    "$$\n",
    "(x \\oplus u)_i = \\sum_{j=1}^{w} x_{i-1+j} u_{k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (x_i,...,x_{i+w-1}) \\dot u\n",
    "$$\n",
    "\n",
    "for insance\n",
    "\n",
    "$$\n",
    "(1,2,3,4) \\oplus (3,2) = (3+4,6+9,9+8) = (7, 12, 17)\n",
    "$$\n",
    "\n",
    "**warning**: this differs from the usual convolution since the kernel and the signal are both visited in increasing index order\n",
    "\n",
    "#### visual represnetation of convolution\n",
    "\n",
    "![conv](./images/conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### common use cases\n",
    "\n",
    "It generalizes naturally to a multi-dimensional input, although specification can become complicated.\n",
    "\n",
    "Its most usual form for \"convolutional networks\" processes a 3d tensor as input to output a 2d tensor. The kernel is not swiped across channels just across rows and columns.\n",
    "\n",
    "Kernels visualized:\n",
    "\n",
    "![kernel](./images/kernel.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a convolution preserves the signal support structure\n",
    "\n",
    "A 1d signal is converted into a 1d signal, a 2d signal into a 2d signal, and neighboring parts of the input signal influence neighboring parts of the output signal\n",
    "\n",
    "A 3d convolution can be used if the channel index has some metric meaning, such as time for a series of grayscale video frames. Otherwise across channels makes no sense\n",
    "\n",
    "We usually refer to one of the channels generated by convolution layer as an **activation map**\n",
    "\n",
    "The sub-area of an input map that influences a component of the output as the **receptive field** of the latter\n",
    "\n",
    "In the context of convolutional networks, a standard liner layer is called a **fully connected** layer since every input influences every output.\n",
    "\n",
    "#### Convolution modules\n",
    "\n",
    "`F.conv2d(input, weight, bias=None, stride=1, padding=0, dilation =1, groups=1)` - implements a 2d convolution, where `weight` contains the kernels and is `D x C x h x w, bias` is of dimension `D, input` is of dimension\n",
    "\n",
    "$$\n",
    "N \\times C \\times H \\times W\n",
    "$$\n",
    "\n",
    "and the result is of dimension\n",
    "\n",
    "$$\n",
    "N \\times D \\times (H-h+1) \\times (W-w+1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([117, 5, 9, 11])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = torch.empty(5,4,2,3).normal_()\n",
    "bias = torch.empty(5).normal_()\n",
    "input1 = torch.empty(117,4,10,13).normal_()\n",
    "output = F.conv2d(input1, weight, bias)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different kinds of filters and how they affect convolution\n",
    "\n",
    "![conv](./images/filters.png)\n",
    "\n",
    "`class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1,groups=1, bias=true)` wraps the convoluton into a `Module` with the kernels and biases as `Parameter` properly randomized at creation.\n",
    "\n",
    "The kernel size is either a pair (`h,w`) or a single value `k` interperted as (`k,k`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### additional parameters\n",
    "\n",
    "Convolutions have three additional parameters\n",
    "\n",
    "- The **padding** specifies the size of a zeroed frame added around the input\n",
    "- The **stride** specifies a step size when moving the kernel acorss the signal\n",
    "- The **dilation** modulates the expansion of the filter without adding weight\n",
    "\n",
    "#### Padding & stride\n",
    "\n",
    "padding is the size of the kernel. In the image a 3x3 kernel is moving\n",
    "\n",
    "stride on the other hand are the dots and how much the kernel frame shifts\n",
    "\n",
    "![padd](./images/padding.png)\n",
    "\n",
    "#### Dilation\n",
    "\n",
    "the dilation modulates the expansion of the filter support by adding rows and columns of zeros between coefficients\n",
    "\n",
    "It is 1 for standard convolutions but can be greater in which case the resulting operation can be envisioned as a convolution with a regularly sparsified filter\n",
    "\n",
    "#### Putting it altogether\n",
    "\n",
    "A convolution with a kernel sized `k` and dilation `d` can be interpreted as a convolution with a filter size `1+(k-1)d` with only `k` non-zero coefficients. \n",
    "\n",
    "For example with `k=3` and `d=4` the difference between the input map size and the output map size is `1+(3-1)4-1=8`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 12, 22])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(1,1,20,30).normal_()\n",
    "l = nn.Conv2d(1,1,kernel_size=3, dilation=4)\n",
    "l(x).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "having a dilation field greater than one increases the units receptive field size without increasing the number of parameters\n",
    "\n",
    "**Convolutions with stride or dilation strictly greater than one reduce the activation map size, for instance to make a final classification decision**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling\n",
    "\n",
    "the historical approach to compute a low dimensional signal (e.g. few scores) froma high dimnesion one (e.g. an image) was to use pooling operations.\n",
    "\n",
    "Such an operation aims at grouping several activations into a single \"more meaningul one\"\n",
    "\n",
    "#### Max Pooling\n",
    "\n",
    "the most standard type of pooling is max-pooling, which computes max values over **non-overlapping blocks**.\n",
    "\n",
    "For instance in 1d with a kernel of size 2:\n",
    "\n",
    "![pool](./images/pool.png)\n",
    "\n",
    "The average pooling computes average values per block instead of max values.\n",
    "\n",
    "#### Why\n",
    "\n",
    "pooling provides invariance to any permutation inside one of the cell.\n",
    "\n",
    "More practically it provides a pseudo invariance to deformation that results into local translations. Cleans up noise, and ignores little changes coming in a signal\n",
    "\n",
    "### PyTorch Modules\n",
    "\n",
    "`F.max_pool2s(input, kernel_size, stride=None, padding=0, dilation=1,ceil_mode=False,return_indices=false)` - takes as input a $N \\times C \\times H \\times W$ tensor and akernel size $(h,w)$ or $k$ interperted as $(k,k)$ applies the max-pooling on each channel of each sample separately and produce if the padding is $0$ a $N \\times C \\times \\lfloor H/h \\rfloor \\times \\lfloor W/w \\rfloor$ output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 0., 2., 1., 1.],\n",
       "          [2., 2., 2., 0., 2., 1.]],\n",
       "\n",
       "         [[1., 1., 0., 0., 2., 1.],\n",
       "          [0., 2., 0., 2., 1., 0.]]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(1,2,2,6).random_(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 2., 1.],\n",
       "          [2., 2., 2.]],\n",
       "\n",
       "         [[1., 0., 2.],\n",
       "          [2., 2., 1.]]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.max_pool2d(x,(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for convolution, pooling operations can be modulated through their stride and padding.\n",
    "\n",
    "While for convolution the default stride is `1`, for pooling it is equal to the kernel size, but this is not obligatory\n",
    "\n",
    "default padding is zero.\n",
    "\n",
    "```\n",
    "class torch.nn.MaxPool2d(kernel_size, stride=None,\n",
    "                         padding=0, dilation=1\n",
    "                         return_indices=False, ceil_mode=False)\n",
    "```\n",
    "\n",
    "Wraps the max-pooling operations into a Module\n",
    "\n",
    "As for convolutions, ther kernel size is either a pair `(h,w)` or a single value `k` interperted as `(k,k)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Writing a PyTorch Module\n",
    "\n",
    "We now have all the bricks needed to build our first convolutional network from scratch. The last technical point is the tensor shape between layers.\n",
    "\n",
    "Both the convolution and pooling layers take as input batches of samples each one being itself a 3d tensor $C \\times H \\times W$\n",
    "\n",
    "The output has the same structure, and tensors have to be explicitly reshaped before being forwarded to fully connected layer\n",
    "\n",
    "#### Example of explicit reshaping of tensors and the amount of parameters/products\n",
    "![re](./images/reshape.png)\n",
    "\n",
    "#### Modules\n",
    "\n",
    "PyTorch offers a sequential container module `torch.nn.Sequential` to build simple architectures\n",
    "\n",
    "For instance a MLP with a `10` dimension, `2` dimension output, ReLU activation and two hidden layers of dimensions `100` and `50` can be written as \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(10,100),nn.ReLU(),\n",
    "    nn.Linear(100,50), nn.ReLU(),\n",
    "    nn.Linear(50,2)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However for any model of reasonable complexity the best it to write a sub-class of `torch.nn.Module`\n",
    "\n",
    "#### How to create a Module\n",
    "\n",
    "To create a `Module`, one has to inherit from the base class and implement the constructor `__init__(self,...)`  and the forward pass `forward(self,x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32,64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256,200)\n",
    "        self.fc2 = nn.Linear(200,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x),kernel_size =3, stride=3))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x),kernel_size =2, stride=2))\n",
    "        x = x.view(-1,256)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x= self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inheriting from `torch.nn.Module` provides many mechanisms implemented in the superclass\n",
    "\n",
    "First the `(...)` operator is redefined to call the `forward(...)` method and run additional operations. The forward pass should be executed through this operator, and not by calling `forward` explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 10])\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "input1 = torch.empty(12,1,28,28).normal_()\n",
    "output = model(input1)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the `Parameters` added as class attributes, or from modules added as class attributes, are seen by `Module.paramters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 5, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([64, 32, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([200, 256])\n",
      "torch.Size([200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "for k in model.parameters():\n",
    "    print(k.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**warning**: Parameters included in dictionaries and lists are not seen.\n",
    "\n",
    "To combat this a simple solution is to add `torch.nn.ModuleList` which is a list of modules properly dealth with by PyTorch's machinery\n",
    "\n",
    "As long as you use autograd-compliant operations, the backward pass is implemented automatically\n",
    "\n",
    "This is crucial to allow the optimization of the `Parameters` with gradient descent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
